
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Crawling logic Â· GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
        <link rel="stylesheet" href="../styles/website.css">
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="messaging.html" />
    
    
    <link rel="prev" href="fetcher.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../chapter0/">
            
                <a href="../chapter0/">
            
                    
                    Project setup
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="../chapter0/installation.html">
            
                <a href="../chapter0/installation.html">
            
                    
                    Go installation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="../chapter0/environment.html">
            
                <a href="../chapter0/environment.html">
            
                    
                    Editors and IDEs
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="./">
            
                <a href="./">
            
                    
                    Web crawler
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="fetcher.html">
            
                <a href="fetcher.html">
            
                    
                    Fetch and parse
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.3.2" data-path="crawling-logic.html">
            
                <a href="crawling-logic.html">
            
                    
                    Crawling logic
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="messaging.html">
            
                <a href="messaging.html">
            
                    
                    Introducing a middleware
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Crawling logic</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="the-crawling-logic">The crawling logic</h1>
<p>We&apos;re at the core logic of the web crawler, the algorithm is simple but we have
to define some rules and settings to manage the crawling process and also to
cover up some corner cases as well. For example what to do if we&apos;re making too
many calls to the server? What if the response time gets higher
at every call? How to decide the user-agent to adopt for each call? These are
only some of the questions that arises during the design of our crawler.</p>
<h2 id="crawling-settings">Crawling settings</h2>
<p>Let&apos;s start simple by creating a struct carrying a state in the form of
crawling settings, defining the behavior of the crawler.</p>
<p>What we want to be able to configure is:</p>
<ul>
<li><code>fetchingTimeout</code>, the number of (m)s to wait before declaring the link
unreachable if it&apos;s hanging</li>
<li><code>crawlingTimeout</code>, the number of (m)s to wait after the last link we found
in the last crawled page, after which we declare the crawling done</li>
<li><code>politenessDelay</code>, the number of (m)s to wait after an HTTP request before
making another under the same domain</li>
<li><code>maxDepth</code>, the number of level to crawl through a domain tree, some sites
can be several levels deep, it comes handy set a limit in this terms</li>
<li><code>maxConcurrency</code>, the number of concurrent worker goroutines that may run
during the crawling process of a domain, this is utmost important</li>
<li><code>userAgent</code>, the user-agent that we declare on every HTTP request header,
this is also an important setting, being polite when crawling a domain allow
the server to know who&apos;s visiting every link and also define some
&quot;rules of the house&quot; to be applied in order to avoid being banned</li>
</ul>
<h3 id="limiting-the-concurrency">Limiting the concurrency</h3>
<p>Setting an upper limit on the number of concurrent goroutines is vital in this
applications and generally that limit isn&apos;t even that high, some <code>robots.txt</code>
(those &quot;rules of the house&quot; defined by the domain) often set a politeness delay
of over 1 min, or you start receive lot of <code>429: Too many requests</code> responses
or <code>503: Service unavailable</code> with a <code>Retry-After</code> header set. This makes
harder than it seems to write a good crawling algorithm, in this case we start
simple by setting a fixed politeness delay and a concurrency limit, going
incremental we&apos;ll try to implement some sort of heuristic to take into account
the response time of each call to adjust the delay and also the <code>robots.txt</code>
rules if present.</p>
<p>The other concerns about letting unlimited concurrency are the most known
regarding the resources of the host machine:</p>
<ul>
<li>goroutines are cheap and can be spawned in millions, each goroutine has a
memory cost around 2-5 Kbs, clearly those millions would put a toll in terms
of memory usage</li>
<li>the number of opened state file descriptors grows really fast, under Linux
without increasing it with <code>ulimit n</code> the limit is reached pretty fast as
every TCP connection relies on a socket; this is especially true if the
crawler needs also to maintain a pool of connections to a DB or store data
on disk</li>
</ul>
<p><strong>crawler.go</strong></p>
<pre><code class="lang-go"><span class="hljs-comment">// Package crawler containing the crawling logics and utilities to scrape</span>
<span class="hljs-comment">// remote resources on the web</span>
<span class="hljs-keyword">package</span> crawler

<span class="hljs-keyword">import</span> (
    <span class="hljs-string">&quot;context&quot;</span>
    <span class="hljs-string">&quot;encoding/json&quot;</span>
    <span class="hljs-string">&quot;log&quot;</span>
    <span class="hljs-string">&quot;net/url&quot;</span>
    <span class="hljs-string">&quot;os&quot;</span>
    <span class="hljs-string">&quot;os/signal&quot;</span>
    <span class="hljs-string">&quot;sync&quot;</span>
    <span class="hljs-string">&quot;sync/atomic&quot;</span>
    <span class="hljs-string">&quot;syscall&quot;</span>
    <span class="hljs-string">&quot;time&quot;</span>

    <span class="hljs-string">&quot;github.com/codepr/webcrawler/fetcher&quot;</span>
)

<span class="hljs-keyword">const</span> (
    <span class="hljs-comment">// Default fetcher timeout before giving up an URL</span>
    defaultFetchTimeout time.Duration = <span class="hljs-number">10</span> * time.Second
    <span class="hljs-comment">// Default crawling timeout, time to wait to stop the crawl after no links are</span>
    <span class="hljs-comment">// found</span>
    defaultCrawlingTimeout time.Duration = <span class="hljs-number">30</span> * time.Second
    <span class="hljs-comment">// Default politeness delay, fixed delay to calculate a randomized wait time</span>
    <span class="hljs-comment">// for subsequent HTTP calls to a domain</span>
    defaultPolitenessDelay time.Duration = <span class="hljs-number">500</span> * time.Millisecond
    <span class="hljs-comment">// Default depth to crawl for each domain</span>
    defaultDepth <span class="hljs-keyword">int</span> = <span class="hljs-number">16</span>
    <span class="hljs-comment">// Default number of concurrent goroutines to crawl</span>
    defaultConcurrency <span class="hljs-keyword">int</span> = <span class="hljs-number">8</span>
    <span class="hljs-comment">// Default user agent to use</span>
    defaultUserAgent <span class="hljs-keyword">string</span> = <span class="hljs-string">&quot;Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)&quot;</span>
)

<span class="hljs-comment">// ParsedResult contains the URL crawled and an array of links found, json</span>
<span class="hljs-comment">// serializable to be sent on message queues</span>
<span class="hljs-keyword">type</span> ParsedResult <span class="hljs-keyword">struct</span> {
    URL   <span class="hljs-keyword">string</span>   <span class="hljs-string">`json:&quot;url&quot;`</span>
    Links []<span class="hljs-keyword">string</span> <span class="hljs-string">`json:&quot;links&quot;`</span>
}

<span class="hljs-comment">// CrawlerSettings represents general settings for the crawler and his</span>
<span class="hljs-comment">// dependencies</span>
<span class="hljs-keyword">type</span> CrawlerSettings <span class="hljs-keyword">struct</span> {
    <span class="hljs-comment">// FetchingTimeout is the time to wait before closing a connection that does not</span>
    <span class="hljs-comment">// respond</span>
    FetchingTimeout time.Duration
    <span class="hljs-comment">// CrawlingTimeout is the number of second to wait before exiting the crawling</span>
    <span class="hljs-comment">// in case of no links found</span>
    CrawlingTimeout time.Duration
    <span class="hljs-comment">// Concurrency is the number of concurrent goroutine to run while fetching</span>
    <span class="hljs-comment">// a page. 0 means unbounded</span>
    Concurrency <span class="hljs-keyword">int</span>
    <span class="hljs-comment">// Parser is a `fetcher.Parser` instance object used to parse fetched pages</span>
    Parser fetcher.Parser
    <span class="hljs-comment">// MaxDepth represents a limit on the number of pages recursively fetched.</span>
    <span class="hljs-comment">// 0 means unlimited</span>
    MaxDepth <span class="hljs-keyword">int</span>
    <span class="hljs-comment">// UserAgent is the user-agent header set in each GET request, most of the</span>
    <span class="hljs-comment">// times it also defines which robots.txt rules to follow while crawling a</span>
    <span class="hljs-comment">// domain, depending on the directives specified by the site admin</span>
    UserAgent <span class="hljs-keyword">string</span>
    <span class="hljs-comment">// PolitenessFixedDelay represents the delay to wait between subsequent</span>
    <span class="hljs-comment">// calls to the same domain, it&apos;ll taken into consideration against a</span>
    <span class="hljs-comment">// robots.txt if present and against the last response time, taking always</span>
    <span class="hljs-comment">// the major between these last two. Robots.txt has the precedence.</span>
    PolitenessFixedDelay time.Duration
}

<span class="hljs-comment">// WebCrawler is the main object representing a crawler</span>
<span class="hljs-keyword">type</span> WebCrawler <span class="hljs-keyword">struct</span> {
    <span class="hljs-comment">// logger is a private logger instance</span>
    logger *log.Logger
    <span class="hljs-comment">// settings is a pointer to `CrawlerSettings` containing some crawler</span>
    <span class="hljs-comment">// specifications</span>
    settings *CrawlerSettings
}

<span class="hljs-comment">// New create a new Crawler instance, accepting a maximum level of depth during</span>
<span class="hljs-comment">// crawling all the anchor links inside each page, a concurrency limiter that</span>
<span class="hljs-comment">// defines how many goroutine to run in parallel while fetching links and a</span>
<span class="hljs-comment">// timeout for each HTTP call.</span>
<span class="hljs-keyword">func</span> New(userAgent <span class="hljs-keyword">string</span>) *WebCrawler {
    <span class="hljs-comment">// Default crawler settings</span>
    settings := &amp;CrawlerSettings{
        FetchingTimeout:      defaultFetchTimeout,
        Parser:               fetcher.NewGoqueryParser(),
        UserAgent:            userAgent,
        CrawlingTimeout:      defaultCrawlingTimeout,
        PolitenessFixedDelay: defaultPolitenessDelay,
        Concurrency:          defaultConcurrency,
    }
    crawler := &amp;WebCrawler{
        logger:   log.New(os.Stderr, <span class="hljs-string">&quot;crawler: &quot;</span>, log.LstdFlags),
        settings: settings,
    }
    <span class="hljs-keyword">return</span> crawler
}

<span class="hljs-comment">// NewFromSettings create a new webCrawler with the settings passed in</span>
<span class="hljs-keyword">func</span> NewFromSettings(settings *CrawlerSettings) *WebCrawler {
    <span class="hljs-keyword">return</span> &amp;WebCrawler{
        logger:   log.New(os.Stderr, <span class="hljs-string">&quot;crawler: &quot;</span>, log.LstdFlags),
        settings: settings,
    }
}
</code></pre>
<p>As we can see, the number of settings is high and a little uncomfortable to
manage with constructor functions, this is a good case to adopt an opt pattern,
a common build pattern in Go, let&apos;s modify the <code>New</code> function:</p>
<p><strong>crawler.go</strong></p>
<pre><code class="lang-go"><span class="hljs-comment">// CrawlerOpt is a type definition for option pattern while creating a new</span>
<span class="hljs-comment">// crawler</span>
<span class="hljs-keyword">type</span> CrawlerOpt <span class="hljs-keyword">func</span>(*CrawlerSettings)

<span class="hljs-comment">// New create a new Crawler instance, accepting a maximum level of depth during</span>
<span class="hljs-comment">// crawling all the anchor links inside each page, a concurrency limiter that</span>
<span class="hljs-comment">// defines how many goroutine to run in parallel while fetching links and a</span>
<span class="hljs-comment">// timeout for each HTTP call.</span>
<span class="hljs-keyword">func</span> New(userAgent <span class="hljs-keyword">string</span>, opts ...CrawlerOpt) *WebCrawler {
    <span class="hljs-comment">// Default crawler settings</span>
    settings := &amp;CrawlerSettings{
        FetchingTimeout:      defaultFetchTimeout,
        Parser:               fetcher.NewGoqueryParser(),
        UserAgent:            userAgent,
        CrawlingTimeout:      defaultCrawlingTimeout,
        PolitenessFixedDelay: defaultPolitenessDelay,
        Concurrency:          defaultConcurrency,
    }
    <span class="hljs-comment">// Mix in all optionals</span>
    <span class="hljs-keyword">for</span> _, opt := <span class="hljs-keyword">range</span> opts {
        opt(settings)
    }
    crawler := &amp;WebCrawler{
        logger:   log.New(os.Stderr, <span class="hljs-string">&quot;crawler: &quot;</span>, log.LstdFlags),
        settings: settings,
    }
    <span class="hljs-keyword">return</span> crawler
}
</code></pre>
<p>Now the constructor accepts optional parameters in the form of a factory
function, this makes possible to customize the creation of the <code>WebCrawler</code>
object:</p>
<pre><code class="lang-go"><span class="hljs-comment">// withConcurrency is a simple constructor option to pass into the</span>
<span class="hljs-comment">// crawler.New function call to set the concurrency level</span>
<span class="hljs-keyword">func</span> withConcurrency(concurrency <span class="hljs-keyword">int</span>) crawler.CrawlerOpt {
    <span class="hljs-keyword">return</span> <span class="hljs-keyword">func</span>(s *crawler.CrawlerSettings) {
        s.Concurrency = concurrency
    }
}
c := crawler.New(<span class="hljs-string">&quot;user-agent&quot;</span>, withConcurrency(<span class="hljs-number">4</span>))
</code></pre>
<h2 id="crawling-a-domain">Crawling a domain</h2>
<p>The core of the crawler component, we want to export just one simple function
that allows us to start the crawling process on one or more domains,
practically we can see the application flow as a coordinator-helpers hierarchy
consisting of two simple functions:</p>
<ul>
<li><code>Crawl</code> the only exported function, accepts a variadic number of URL strings
and spawn a <code>crawlPage</code> goroutine for each of them</li>
<li><code>crawlPage</code> private function, contains the core logic, for each URL in the
queue extracts every link found and put them into the same queue, starting
from the upper-most link passed in by the <code>Crawl</code> function</li>
</ul>
<p>As we already seen, the problem is easily solved recursively, but Go provides
us tools to avoid the use of recursion which is generally a prerogative of
functional languages and those that provide tail-recursion optimization (see
Scala, Haskell or Erlang for example).</p>
<p>We want to use an unbuffered channel as our queue for every new URL we want to
crawl, this also allows to spawn a worker goroutine for each URL and push all
extracted URLs in each page directly into the channel queue, governed by the
main routine</p>
<p><strong>crawler.go</strong></p>
<pre><code class="lang-go"><span class="hljs-comment">// Crawl a single page by fetching the starting URL, extracting all anchors</span>
<span class="hljs-comment">// and exploring each one of them applying the same steps. Every image link</span>
<span class="hljs-comment">// found is forwarded into a dedicated channel, as well as errors.</span>
<span class="hljs-comment">//</span>
<span class="hljs-comment">// A waitgroup is used to synchronize it&apos;s execution, enabling the caller to</span>
<span class="hljs-comment">// wait for completion.</span>
<span class="hljs-keyword">func</span> (c *WebCrawler) crawlPage(rootURL *url.URL, wg *sync.WaitGroup, ctx context.Context) {
    <span class="hljs-comment">// First we wanna make sure we decrease the waitgroup counter at the end of</span>
    <span class="hljs-comment">// the crawling</span>
    <span class="hljs-keyword">defer</span> wg.Done()
    fetchClient := fetcher.New(c.settings.UserAgent,
        c.settings.Parser, c.settings.FetchingTimeout)
    <span class="hljs-keyword">var</span> (
        <span class="hljs-comment">// semaphore is just a value-less channel used to limit the number of</span>
        <span class="hljs-comment">// concurrent goroutine workers fetching links</span>
        semaphore <span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>{}
        <span class="hljs-comment">// New found links channel</span>
        linksCh <span class="hljs-keyword">chan</span> []*url.URL
        stop    <span class="hljs-keyword">bool</span>
        depth   <span class="hljs-keyword">int</span>
        <span class="hljs-comment">// A map is used to track all visited links, in order to avoid multiple</span>
        <span class="hljs-comment">// fetches on the previous visited links</span>
        seen    <span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]<span class="hljs-keyword">bool</span> = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]<span class="hljs-keyword">bool</span>)
        fetchWg sync.WaitGroup  = sync.WaitGroup{}
        <span class="hljs-comment">// An atomic counter to make sure that we&apos;ve already crawled all remaining</span>
        <span class="hljs-comment">// links if a timeout occur</span>
        linkCounter <span class="hljs-keyword">int32</span> = <span class="hljs-number">1</span>
    )
    <span class="hljs-comment">// Set the concurrency level by using a buffered channel as semaphore</span>
    <span class="hljs-keyword">if</span> c.settings.Concurrency &gt; <span class="hljs-number">0</span> {
        semaphore = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>{}, c.settings.Concurrency)
        linksCh = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> []*url.URL, c.settings.Concurrency)
    } <span class="hljs-keyword">else</span> {
        <span class="hljs-comment">// we want to disallow the unlimited concurrency, to avoid being banned from</span>
        <span class="hljs-comment">// the ccurrent crawled domain and also to avoid running OOM or running out</span>
        <span class="hljs-comment">// of unix file descriptors, as each HTTP call is built upon a  socket</span>
        <span class="hljs-comment">// connection, which is in-fact an opened descriptor.</span>
        semaphore = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>{}, <span class="hljs-number">1</span>)
        linksCh = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> []*url.URL, <span class="hljs-number">1</span>)
    }
    <span class="hljs-comment">// Just a kickstart for the first URL to scrape</span>
    linksCh &lt;- []*url.URL{rootURL}
    <span class="hljs-comment">// Every cycle represents a single page crawling, when new anchors are</span>
    <span class="hljs-comment">// found, the counter is increased, making the loop continue till the</span>
    <span class="hljs-comment">// end of links</span>
    <span class="hljs-keyword">for</span> !stop {
        <span class="hljs-keyword">select</span> {
        <span class="hljs-keyword">case</span> links := &lt;-linksCh:
            <span class="hljs-keyword">for</span> _, link := <span class="hljs-keyword">range</span> links {
                <span class="hljs-comment">// Skip already visited links</span>
                <span class="hljs-keyword">if</span> seen[link.String()] {
                    atomic.AddInt32(&amp;linkCounter, <span class="hljs-number">-1</span>)
                    <span class="hljs-keyword">continue</span>
                }
                seen[link.String()] = <span class="hljs-literal">true</span>
                <span class="hljs-comment">// Spawn a goroutine to fetch the link, throttling by</span>
                <span class="hljs-comment">// concurrency argument on the semaphore will take care of the</span>
                <span class="hljs-comment">// concurrent number of goroutine.</span>
                fetchWg.Add(<span class="hljs-number">1</span>)
                <span class="hljs-keyword">go</span> <span class="hljs-keyword">func</span>(link *url.URL, stopSentinel <span class="hljs-keyword">bool</span>, w *sync.WaitGroup) {
                    <span class="hljs-keyword">defer</span> w.Done()
                    <span class="hljs-keyword">defer</span> atomic.AddInt32(&amp;linkCounter, <span class="hljs-number">-1</span>)
                    <span class="hljs-comment">// 0 concurrency level means we serialize calls as</span>
                    <span class="hljs-comment">// goroutines are cheap but not that cheap (around 2-5 kb</span>
                    <span class="hljs-comment">// each, 1 million links = ~4/5 GB ram), by allowing for</span>
                    <span class="hljs-comment">// unlimited number of workers, potentially we could run</span>
                    <span class="hljs-comment">// OOM (or banned from the website) really fast</span>
                    semaphore &lt;- <span class="hljs-keyword">struct</span>{}{}
                    <span class="hljs-keyword">defer</span> <span class="hljs-keyword">func</span>() {
                        time.Sleep(c.settings.PolitenessFixedDelay)
                        &lt;-semaphore
                    }()
                    <span class="hljs-comment">// We fetch the current link here and parse HTML for children links</span>
                    responseTime, foundLinks, err := fetchClient.FetchLinks(link.String())
                    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
                        c.logger.Println(err)
                        <span class="hljs-keyword">return</span>
                    }
                    <span class="hljs-comment">// No errors occured, we want to enqueue all scraped links</span>
                    <span class="hljs-comment">// to the link queue</span>
                    <span class="hljs-keyword">if</span> stopSentinel || foundLinks == <span class="hljs-literal">nil</span> || <span class="hljs-built_in">len</span>(foundLinks) == <span class="hljs-number">0</span> {
                        <span class="hljs-keyword">return</span>
                    }
                    atomic.AddInt32(&amp;linkCounter, <span class="hljs-keyword">int32</span>(<span class="hljs-built_in">len</span>(foundLinks)))
                    <span class="hljs-comment">// Enqueue found links for the next cycle</span>
                    linksCh &lt;- foundLinks
                }(link, stop, &amp;fetchWg)
                <span class="hljs-comment">// We want to check if a level limit is set and in case, check if</span>
                <span class="hljs-comment">// it&apos;s reached as every explored link count as a level</span>
                <span class="hljs-keyword">if</span> c.settings.MaxDepth == <span class="hljs-number">0</span> || !stop {
                    depth++
                    stop = c.settings.MaxDepth &gt; <span class="hljs-number">0</span> &amp;&amp; depth &gt;= c.settings.MaxDepth
                }
            }
        <span class="hljs-keyword">case</span> &lt;-time.After(c.settings.CrawlingTimeout):
            <span class="hljs-comment">// c.settings.CrawlingTimeout seconds without any new link found, check</span>
            <span class="hljs-comment">// that the remaining links have been processed and stop the iteration</span>
            <span class="hljs-keyword">if</span> atomic.LoadInt32(&amp;linkCounter) &lt;= <span class="hljs-number">0</span> {
                stop = <span class="hljs-literal">true</span>
            }
        <span class="hljs-keyword">case</span> &lt;-ctx.Done():
            <span class="hljs-keyword">return</span>
        }
    }
    fetchWg.Wait()
}

<span class="hljs-comment">// Crawl will walk through a list of URLs spawning a goroutine for each one of</span>
<span class="hljs-comment">// them</span>
<span class="hljs-keyword">func</span> (c *WebCrawler) Crawl(URLs ...<span class="hljs-keyword">string</span>) {
    wg := sync.WaitGroup{}
    ctx, cancel := context.WithCancel(context.Background())
    <span class="hljs-comment">// Sanity check for URLs passed, check that they&apos;re in the form</span>
    <span class="hljs-comment">// scheme://host:port/path, adding missing fields</span>
    <span class="hljs-keyword">for</span> _, href := <span class="hljs-keyword">range</span> URLs {
        url, err := url.Parse(href)
        <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
            log.Fatal(err)
        }
        <span class="hljs-keyword">if</span> url.Scheme == <span class="hljs-string">&quot;&quot;</span> {
            url.Scheme = <span class="hljs-string">&quot;https&quot;</span>
        }
        <span class="hljs-comment">// Spawn a goroutine for each URLs to crawl, a waitgroup is used to wait</span>
        <span class="hljs-comment">// for completion</span>
        wg.Add(<span class="hljs-number">1</span>)
        <span class="hljs-keyword">go</span> c.crawlPage(url, &amp;wg, ctx)
    }
    <span class="hljs-comment">// Graceful shutdown of workers</span>
    signalCh := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> os.Signal, <span class="hljs-number">1</span>)
    signal.Notify(signalCh, os.Interrupt, syscall.SIGTERM)
    <span class="hljs-keyword">go</span> <span class="hljs-keyword">func</span>() {
        &lt;-signalCh
        cancel()
        os.Exit(<span class="hljs-number">1</span>)
    }()
    wg.Wait()
    c.logger.Println(<span class="hljs-string">&quot;Crawling done&quot;</span>)
}
</code></pre>
<p>Although <code>crawlPage</code> is admittedly a little complex and there&apos;s room for
improvements, the flow is indeed simple and came out as a variation of the
previously thought steps:</p>
<ol>
<li>enqueue start URL in the channel as a list of 1 element</li>
<li>dequeue the link-list from the queue</li>
<li>for each link in the list<ul>
<li>check the ephemeral state of the crawling to see if we already have
visited the URL</li>
<li>fetch HTML contents and extracts all links from the page</li>
<li>enqueue every found link into the channel queue</li>
</ul>
</li>
<li>goto 2</li>
</ol>
<p>The <code>select</code> makes it a little harder to follow as it introduces asynchronicity
in the flow, but the low number of branches, just two, makes it simple to
understand: In the second branch it just starts a timer everytime &quot;nothing
happens&quot; in the main channel queue, in other word if no links are found for a
determined amount of time (<code>crawlingTimeout</code>) the loop gets interrupted and the
crawling for that domain ends.</p>
<p>As of now however, it&apos;s difficult to unit-test, we have to find out a way to
make the business logic less tied to the application and above all make it
possible to forward crawling results to outside, so that external clients may
use them without being tightly coupled with the crawler.
That way it&apos;d be possible to perform an opaque-box testing, without worrying
of what&apos;s happening inside the crawling loop.</p>
<p>We&apos;ve just added two additional <code>.go</code> files for the <code>crawler</code> package, one for
unit tests and one for source, this the current project structure so far:</p>
<pre><code class="lang-sh">tree
.
&#x251C;&#x2500;&#x2500; crawler.go
&#x251C;&#x2500;&#x2500; crawler_test.go
&#x251C;&#x2500;&#x2500; fetcher
&#x2502;   &#x251C;&#x2500;&#x2500; fetcher.go
&#x2502;   &#x251C;&#x2500;&#x2500; fetcher_test.go
&#x2502;   &#x251C;&#x2500;&#x2500; parser.go
&#x2502;   &#x2514;&#x2500;&#x2500; parser_test.go
&#x251C;&#x2500;&#x2500; go.mod
&#x2514;&#x2500;&#x2500; go.sum
</code></pre>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="fetcher.html" class="navigation navigation-prev " aria-label="Previous page: Fetch and parse">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="messaging.html" class="navigation navigation-next " aria-label="Next page: Introducing a middleware">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Crawling logic","level":"1.3.2","depth":2,"next":{"title":"Introducing a middleware","level":"1.3.3","depth":2,"path":"chapter1/messaging.md","ref":"chapter1/messaging.md","articles":[]},"previous":{"title":"Fetch and parse","level":"1.3.1","depth":2,"path":"chapter1/fetcher.md","ref":"chapter1/fetcher.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["diff","livereload"],"pluginsConfig":{"diff":{"type":"markdown","method":"diffChars","options":{}},"livereload":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css"}},"file":{"path":"chapter1/crawling-logic.md","mtime":"2020-11-16T20:07:00.233Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2020-11-16T19:33:49.329Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

