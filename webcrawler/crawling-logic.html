
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Crawling logic Â· GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
        <link rel="stylesheet" href="../styles/website.css">
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    
    <link rel="prev" href="fetcher.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../project_setup/">
            
                <a href="../project_setup/">
            
                    
                    Project setup
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="../project_setup/installation.html">
            
                <a href="../project_setup/installation.html">
            
                    
                    Go installation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="../project_setup/environment.html">
            
                <a href="../project_setup/environment.html">
            
                    
                    Editors and IDEs
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="./">
            
                <a href="./">
            
                    
                    Webcrawler
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="fetcher.html">
            
                <a href="fetcher.html">
            
                    
                    Fetch and parse
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.3.2" data-path="crawling-logic.html">
            
                <a href="crawling-logic.html">
            
                    
                    Crawling logic
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Crawling logic</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="the-crawling-logic">The crawling logic</h1>
<p>We&apos;re at the core logic of the web crawler, the algorithm is simple but we have
to define some rules and settings to manage the crawling process and also to
cover up some corner cases as well. For example what to do if we&apos;re making too
many calls to the server? What if the response time become more and more higher
at every call? How to decide the user-agent to adopt for each call? These are
only some of the questions that arises during the design of our crawler.</p>
<p>Let&apos;s start simple by creating a struct carrying a state in the form of
crawling settings, defining the behavior of the crawler, like the timeout to
apply on each call, the maximum depth to reach on every domain we crawl
(potentially a domain tree could go down several levels)</p>
<p><strong>crawler.go</strong></p>
<pre><code class="lang-go"><span class="hljs-comment">// Package crawler containing the crawling logics and utilities to scrape</span>
<span class="hljs-comment">// remote resources on the web</span>
<span class="hljs-keyword">package</span> crawler

<span class="hljs-keyword">import</span> (
    <span class="hljs-string">&quot;context&quot;</span>
    <span class="hljs-string">&quot;encoding/json&quot;</span>
    <span class="hljs-string">&quot;log&quot;</span>
    <span class="hljs-string">&quot;net/url&quot;</span>
    <span class="hljs-string">&quot;os&quot;</span>
    <span class="hljs-string">&quot;os/signal&quot;</span>
    <span class="hljs-string">&quot;sync&quot;</span>
    <span class="hljs-string">&quot;sync/atomic&quot;</span>
    <span class="hljs-string">&quot;syscall&quot;</span>
    <span class="hljs-string">&quot;time&quot;</span>

    <span class="hljs-string">&quot;github.com/codepr/webcrawler/fetcher&quot;</span>
)

<span class="hljs-keyword">const</span> (
    <span class="hljs-comment">// Default fetcher timeout before giving up an URL</span>
    defaultFetchTimeout time.Duration = <span class="hljs-number">10</span> * time.Second
    <span class="hljs-comment">// Default crawling timeout, time to wait to stop the crawl after no links are</span>
    <span class="hljs-comment">// found</span>
    defaultCrawlingTimeout time.Duration = <span class="hljs-number">30</span> * time.Second
    <span class="hljs-comment">// Default depth to crawl for each domain</span>
    defaultDepth <span class="hljs-keyword">int</span> = <span class="hljs-number">16</span>
    <span class="hljs-comment">// Default number of concurrent goroutines to crawl</span>
    defaultConcurrency <span class="hljs-keyword">int</span> = <span class="hljs-number">8</span>
    <span class="hljs-comment">// Default user agent to use</span>
    defaultUserAgent <span class="hljs-keyword">string</span> = <span class="hljs-string">&quot;Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)&quot;</span>
)

<span class="hljs-comment">// ParsedResult contains the URL crawled and an array of links found, json</span>
<span class="hljs-comment">// serializable to be sent on message queues</span>
<span class="hljs-keyword">type</span> ParsedResult <span class="hljs-keyword">struct</span> {
    URL   <span class="hljs-keyword">string</span>   <span class="hljs-string">`json:&quot;url&quot;`</span>
    Links []<span class="hljs-keyword">string</span> <span class="hljs-string">`json:&quot;links&quot;`</span>
}

<span class="hljs-comment">// CrawlerSettings represents general settings for the crawler and his</span>
<span class="hljs-comment">// dependencies</span>
<span class="hljs-keyword">type</span> CrawlerSettings <span class="hljs-keyword">struct</span> {
    <span class="hljs-comment">// FetchingTimeout is the time to wait before closing a connection that does not</span>
    <span class="hljs-comment">// respond</span>
    FetchingTimeout time.Duration
    <span class="hljs-comment">// CrawlingTimeout is the number of second to wait before exiting the crawling</span>
    <span class="hljs-comment">// in case of no links found</span>
    CrawlingTimeout time.Duration
    <span class="hljs-comment">// Concurrency is the number of concurrent goroutine to run while fetching</span>
    <span class="hljs-comment">// a page. 0 means unbounded</span>
    Concurrency <span class="hljs-keyword">int</span>
    <span class="hljs-comment">// Parser is a `fetcher.Parser` instance object used to parse fetched pages</span>
    Parser fetcher.Parser
    <span class="hljs-comment">// MaxDepth represents a limit on the number of pages recursively fetched.</span>
    <span class="hljs-comment">// 0 means unlimited</span>
    MaxDepth <span class="hljs-keyword">int</span>
    <span class="hljs-comment">// UserAgent is the user-agent header set in each GET request, most of the</span>
    <span class="hljs-comment">// times it also defines which robots.txt rules to follow while crawling a</span>
    <span class="hljs-comment">// domain, depending on the directives specified by the site admin</span>
    UserAgent <span class="hljs-keyword">string</span>
}

<span class="hljs-comment">// WebCrawler is the main object representing a crawler</span>
<span class="hljs-keyword">type</span> WebCrawler <span class="hljs-keyword">struct</span> {
    <span class="hljs-comment">// logger is a private logger instance</span>
    logger *log.Logger
    <span class="hljs-comment">// settings is a pointer to `CrawlerSettings` containing some crawler</span>
    <span class="hljs-comment">// specifications</span>
    settings *CrawlerSettings
}
</code></pre>
<p>As we already seen, the problem is easily solved recursively, but go provide us
with instruments to avoid the use of recursion which is generally a prerogative
of functional languages and those that provide tail-recursion optimization (see
Scala, Haskell or Erlang for example).</p>
<p>We want to use an unbuffered channel as our queue for every new URL we want to
crawl, this also allows to spawn a worker go routine for each URL and push all
extracted URLs in each page directly into the channel queue, governed by the
main routine</p>
<p><strong>crawler.go</strong></p>
<pre><code class="lang-go"><span class="hljs-comment">// New create a new Crawler instance, accepting a maximum level of depth during</span>
<span class="hljs-comment">// crawling all the anchor links inside each page, a concurrency limiter that</span>
<span class="hljs-comment">// defines how many goroutine to run in parallel while fetching links and a</span>
<span class="hljs-comment">// timeout for each HTTP call.</span>
<span class="hljs-keyword">func</span> New(userAgent <span class="hljs-keyword">string</span>) *WebCrawler {
    <span class="hljs-comment">// Default crawler settings</span>
    settings := &amp;CrawlerSettings{
        FetchingTimeout:      defaultFetchTimeout,
        Parser:               fetcher.NewGoqueryParser(),
        UserAgent:            userAgent,
        CrawlingTimeout:      defaultCrawlingTimeout,
        PolitenessFixedDelay: defaultPolitenessDelay,
        Concurrency:          defaultConcurrency,
    }

    crawler := &amp;WebCrawler{
        logger:   log.New(os.Stderr, <span class="hljs-string">&quot;crawler: &quot;</span>, log.LstdFlags),
        settings: settings,
    }

    <span class="hljs-keyword">return</span> crawler
}

<span class="hljs-comment">// NewFromSettings create a new webCrawler with the settings passed in</span>
<span class="hljs-keyword">func</span> NewFromSettings(settings *CrawlerSettings) *WebCrawler {
    <span class="hljs-keyword">return</span> &amp;WebCrawler{
        logger:   log.New(os.Stderr, <span class="hljs-string">&quot;crawler: &quot;</span>, log.LstdFlags),
        settings: settings,
    }
}

<span class="hljs-comment">// Crawl a single page by fetching the starting URL, extracting all anchors</span>
<span class="hljs-comment">// and exploring each one of them applying the same steps. Every image link</span>
<span class="hljs-comment">// found is forwarded into a dedicated channel, as well as errors.</span>
<span class="hljs-comment">//</span>
<span class="hljs-comment">// A waitgroup is used to synchronize it&apos;s execution, enabling the caller to</span>
<span class="hljs-comment">// wait for completion.</span>
<span class="hljs-keyword">func</span> (c *WebCrawler) crawlPage(rootURL *url.URL, wg *sync.WaitGroup, ctx context.Context) {
    <span class="hljs-comment">// First we wanna make sure we decrease the waitgroup counter at the end of</span>
    <span class="hljs-comment">// the crawling</span>
    <span class="hljs-keyword">defer</span> wg.Done()
    fetchClient := fetcher.New(c.settings.UserAgent,
        c.settings.Parser, c.settings.FetchingTimeout)

    <span class="hljs-keyword">var</span> (
        <span class="hljs-comment">// semaphore is just a value-less channel used to limit the number of</span>
        <span class="hljs-comment">// concurrent goroutine workers fetching links</span>
        semaphore <span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>{}
        <span class="hljs-comment">// New found links channel</span>
        linksCh <span class="hljs-keyword">chan</span> []*url.URL
        stop    <span class="hljs-keyword">bool</span>
        depth   <span class="hljs-keyword">int</span>
        <span class="hljs-comment">// A map is used to track all visited links, in order to avoid multiple</span>
        <span class="hljs-comment">// fetches on the previous visited links</span>
        seen    <span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]<span class="hljs-keyword">bool</span> = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-keyword">string</span>]<span class="hljs-keyword">bool</span>)
        fetchWg sync.WaitGroup  = sync.WaitGroup{}
        <span class="hljs-comment">// An atomic counter to make sure that we&apos;ve already crawled all remaining</span>
        <span class="hljs-comment">// links if a timeout occur</span>
        linkCounter <span class="hljs-keyword">int32</span> = <span class="hljs-number">1</span>
    )

    <span class="hljs-comment">// Set the concurrency level by using a buffered channel as semaphore</span>
    <span class="hljs-keyword">if</span> c.settings.Concurrency &gt; <span class="hljs-number">0</span> {
        semaphore = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>{}, c.settings.Concurrency)
        linksCh = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> []*url.URL, c.settings.Concurrency)
    } <span class="hljs-keyword">else</span> {
        <span class="hljs-comment">// we want to disallow the unlimited concurrency, to avoid being banned from</span>
        <span class="hljs-comment">// the ccurrent crawled domain and also to avoid running OOM or running out</span>
        <span class="hljs-comment">// of unix file descriptors, as each HTTP call is built upon a  socket</span>
        <span class="hljs-comment">// connection, which is in-fact an opened descriptor.</span>
        semaphore = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>{}, <span class="hljs-number">1</span>)
        linksCh = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> []*url.URL, <span class="hljs-number">1</span>)
    }

    <span class="hljs-comment">// Just a kickstart for the first URL to scrape</span>
    linksCh &lt;- []*url.URL{rootURL}

    <span class="hljs-comment">// Every cycle represents a single page crawling, when new anchors are</span>
    <span class="hljs-comment">// found, the counter is increased, making the loop continue till the</span>
    <span class="hljs-comment">// end of links</span>
    <span class="hljs-keyword">for</span> !stop {
        <span class="hljs-keyword">select</span> {
        <span class="hljs-keyword">case</span> links := &lt;-linksCh:
            <span class="hljs-keyword">for</span> _, link := <span class="hljs-keyword">range</span> links {
                <span class="hljs-comment">// Skip already visited links or disallowed ones by the robots.txt rules</span>
                <span class="hljs-keyword">if</span> seen[link.String()] || !crawlingRules.Allowed(link) {
                    atomic.AddInt32(&amp;linkCounter, <span class="hljs-number">-1</span>)
                    <span class="hljs-keyword">continue</span>
                }
                seen[link.String()] = <span class="hljs-literal">true</span>
                <span class="hljs-comment">// Spawn a goroutine to fetch the link, throttling by</span>
                <span class="hljs-comment">// concurrency argument on the semaphore will take care of the</span>
                <span class="hljs-comment">// concurrent number of goroutine.</span>
                fetchWg.Add(<span class="hljs-number">1</span>)
                <span class="hljs-keyword">go</span> <span class="hljs-keyword">func</span>(link *url.URL, stopSentinel <span class="hljs-keyword">bool</span>, w *sync.WaitGroup) {
                    <span class="hljs-keyword">defer</span> w.Done()
                    <span class="hljs-keyword">defer</span> atomic.AddInt32(&amp;linkCounter, <span class="hljs-number">-1</span>)
                    <span class="hljs-comment">// 0 concurrency level means we serialize calls as</span>
                    <span class="hljs-comment">// goroutines are cheap but not that cheap (around 2-5 kb</span>
                    <span class="hljs-comment">// each, 1 million links = ~4/5 GB ram), by allowing for</span>
                    <span class="hljs-comment">// unlimited number of workers, potentially we could run</span>
                    <span class="hljs-comment">// OOM (or banned from the website) really fast</span>
                    semaphore &lt;- <span class="hljs-keyword">struct</span>{}{}
                    <span class="hljs-keyword">defer</span> <span class="hljs-keyword">func</span>() {
                        time.Sleep(<span class="hljs-number">1</span>*time.Second)
                        &lt;-semaphore
                    }()
                    <span class="hljs-comment">// We fetch the current link here and parse HTML for children links</span>
                    responseTime, foundLinks, err := fetchClient.FetchLinks(link.String())
                    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
                        c.logger.Println(err)
                        <span class="hljs-keyword">return</span>
                    }
                    <span class="hljs-comment">// No errors occured, we want to enqueue all scraped links</span>
                    <span class="hljs-comment">// to the link queue</span>
                    <span class="hljs-keyword">if</span> stopSentinel || foundLinks == <span class="hljs-literal">nil</span> || <span class="hljs-built_in">len</span>(foundLinks) == <span class="hljs-number">0</span> {
                        <span class="hljs-keyword">return</span>
                    }
                    atomic.AddInt32(&amp;linkCounter, <span class="hljs-keyword">int32</span>(<span class="hljs-built_in">len</span>(foundLinks)))
                    <span class="hljs-comment">// Enqueue found links for the next cycle</span>
                    linksCh &lt;- foundLinks

                }(link, stop, &amp;fetchWg)
                <span class="hljs-comment">// We want to check if a level limit is set and in case, check if</span>
                <span class="hljs-comment">// it&apos;s reached as every explored link count as a level</span>
                <span class="hljs-keyword">if</span> c.settings.MaxDepth == <span class="hljs-number">0</span> || !stop {
                    depth++
                    stop = c.settings.MaxDepth &gt; <span class="hljs-number">0</span> &amp;&amp; depth &gt;= c.settings.MaxDepth
                }
            }
        <span class="hljs-keyword">case</span> &lt;-time.After(c.settings.CrawlingTimeout):
            <span class="hljs-comment">// c.settings.CrawlingTimeout seconds without any new link found, check</span>
            <span class="hljs-comment">// that the remaining links have been processed and stop the iteration</span>
            <span class="hljs-keyword">if</span> atomic.LoadInt32(&amp;linkCounter) &lt;= <span class="hljs-number">0</span> {
                stop = <span class="hljs-literal">true</span>
            }
        <span class="hljs-keyword">case</span> &lt;-ctx.Done():
            <span class="hljs-keyword">return</span>
        }
    }
    fetchWg.Wait()
}

<span class="hljs-comment">// Crawl will walk through a list of URLs spawning a goroutine for each one of</span>
<span class="hljs-comment">// them</span>
<span class="hljs-keyword">func</span> (c *WebCrawler) Crawl(URLs ...<span class="hljs-keyword">string</span>) {
    wg := sync.WaitGroup{}
    ctx, cancel := context.WithCancel(context.Background())
    <span class="hljs-comment">// Sanity check for URLs passed, check that they&apos;re in the form</span>
    <span class="hljs-comment">// scheme://host:port/path, adding missing fields</span>
    <span class="hljs-keyword">for</span> _, href := <span class="hljs-keyword">range</span> URLs {
        url, err := url.Parse(href)
        <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
            log.Fatal(err)
        }
        <span class="hljs-keyword">if</span> url.Scheme == <span class="hljs-string">&quot;&quot;</span> {
            url.Scheme = <span class="hljs-string">&quot;https&quot;</span>
        }
        <span class="hljs-comment">// Spawn a goroutine for each URLs to crawl, a waitgroup is used to wait</span>
        <span class="hljs-comment">// for completion</span>
        wg.Add(<span class="hljs-number">1</span>)
        <span class="hljs-keyword">go</span> c.crawlPage(url, &amp;wg, ctx)

    }
    <span class="hljs-comment">// Graceful shutdown of workers</span>
    signalCh := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> os.Signal, <span class="hljs-number">1</span>)
    signal.Notify(signalCh, os.Interrupt, syscall.SIGTERM)
    <span class="hljs-keyword">go</span> <span class="hljs-keyword">func</span>() {
        &lt;-signalCh
        cancel()
        os.Exit(<span class="hljs-number">1</span>)
    }()
    wg.Wait()
    c.logger.Println(<span class="hljs-string">&quot;Crawling done&quot;</span>)
}
</code></pre>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="fetcher.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page: Fetch and parse">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Crawling logic","level":"1.3.2","depth":2,"previous":{"title":"Fetch and parse","level":"1.3.1","depth":2,"path":"webcrawler/fetcher.md","ref":"webcrawler/fetcher.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["diff","livereload"],"pluginsConfig":{"diff":{"type":"markdown","method":"diffChars","options":{}},"livereload":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css"}},"file":{"path":"webcrawler/crawling-logic.md","mtime":"2020-11-15T23:21:33.354Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2020-11-15T23:50:33.841Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

